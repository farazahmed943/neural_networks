{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level Language Model Using NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset\n",
    "!wget \"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading training data\n",
    "words =  open('/content/names.txt','r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set for the bigrams\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what stoi and itos contains\n",
    "print(stoi)\n",
    "print(itos)\n",
    "\n",
    "# So these basically is a hashmap that contains string to index pairs in one dict and index to string pair in other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [],[]\n",
    "\n",
    "for w in words[:1]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "# Here basically for the first word we are trying to see what the training label and corresponding target label will look like\n",
    "# ix1 contains the corresponding index of first character and ix2 contains the corresponding index of second character\n",
    "# xs is a vector containing [ 0,  5, 13, 13,  1] which is equivalent to [.,e,m,m,a]\n",
    "# ys is a vector containing [ 5, 13, 13,  1,  0] whjch is equivalent to [e,m,m,a,.]\n",
    "# Here since it is target and labels then it basically means when:\n",
    "  # input is 0(.) then target is 5(e)\n",
    "  # input is 5(e) then target is 13(m)\n",
    "  # input is 13(m) then target is 13(m)\n",
    "  # input is 1(a) then target is 0(.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xs)\n",
    "print(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# Here we are doing one hot encoding of the first word i.e .emma\n",
    "# How can we represent a character as vector?\n",
    "# Imagine a vector having 27 dimensions representing each alphabet of english characters\n",
    "# representation of .(dot represents starting of word)-[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "# representation of e - [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0]\n",
    "# representation of m - [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0]\n",
    "# representation of m - [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0]\n",
    "# representation of a - [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0]\n",
    "xenc = F.one_hot(xs,num_classes=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(xenc)\n",
    "# Here we can see the yellow path is where the value is 1 and the vector size is 27."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we want the inputs to be flexible since neural networks doesn't guarentee a integer outputs\n",
    "xenc = F.one_hot(xs,num_classes=27).float()\n",
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling Weights\n",
    "W = torch.randn(27,27)\n",
    "W\n",
    "# torch.randn draws numbers from normal distribution that means most of the numbers will be around zero,\n",
    "# some will be between -3,3 and a very less will be above 3 or below -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the below operation should be\n",
    "# (5,27) * (27,27) = (5,27)\n",
    "xenc @ W\n",
    "# This will multiply the one hot encode vectors with sampled weights\n",
    "# Here we are sending inputs tensor_1 @ tensor_2to 27 neurons and each neurons gets 5 inputs xenc @ W is showing the activations of these\n",
    "# 27 neurons in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see what is the effect on 13th neuron by 3rd input\n",
    "(xenc @ W)[3,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have a task, to get output from these neurons in a way that represents probability\n",
    "# probability has some characteristics that it is alsways positive, and the sum of all probs is 1.\n",
    "# The nueral nets will give us log counts and to convert it to positive will pass it through exponent function,\n",
    "# the exp function has a structure such that if a negative function is input then it will give results below 1 and when a\n",
    "# postiive number occurs then the value can be anything above 1 till inf\n",
    "\n",
    "(xenc @ W).exp()\n",
    "\n",
    "#Above takes above tensor and passes each element through exp function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xenc @ W is called logits\n",
    "logits = xenc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts/counts.sum(1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarising the Code\n",
    "#randomly generating 27 neurons weights and each neuron recieves 27 inputs\n",
    "g = torch.Generator().manual_seed(1)\n",
    "W = torch.randn(27,27,generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs,num_classes=27).float() # input to network -> onehot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts equivalent to N in our Bigram Probabilities Method\n",
    "probs = counts/counts.sum(1,keepdims=True) # probabilities\n",
    "# THE ABOVE TWO LINES ARE NOTHING BUT SOFTMAX FUNCTION WHICH TAKES THE INPUTS AND OUTPUTS PROBABILITIES\n",
    "#SUMMING UP TO 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the word .emma below code is written\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "  input = xs[i].item()\n",
    "  target = ys[i].item()\n",
    "  print(f'Bigram example {i+1} : {itos[input],itos[target]}, index{input,target}')\n",
    "  xenc = F.one_hot(xs[i],num_classes=27).float()\n",
    "  print(f'vector: {xenc}')\n",
    "  g = torch.Generator().manual_seed(2147483647)\n",
    "  W = torch.randn(27,27,generator=g)\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts equivalent to N in our Bigram Probabilities Method\n",
    "  probs = counts/counts.sum(0,keepdims=True) # probabilities\n",
    "  print(f'counts:{counts}')\n",
    "  print(f'probs:{probs}')\n",
    "  print('Label of targExternalet character', itos[target])\n",
    "  p = probs[target]\n",
    "  print('Neural Net Probaility for the label',p.item())\n",
    "  logp = torch.log(p)\n",
    "  print('log likelihood', logp.item())\n",
    "  nll = -logp\n",
    "  print('negative log likelihood', nll.item() )\n",
    "  nlls[i] = nll\n",
    "\n",
    "print(\"****************************************\")\n",
    "print('The average negative log likelihood was', nlls.mean().item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation ----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "xs # input\n",
    "ys # target\n",
    "\n",
    "xenc = F.one_hot(xs,num_classes= 27).float()\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn(27,27,generator = g, requires_grad= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Pass\n",
    "logits = xenc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts/counts.sum(1,keepdims= True)\n",
    "\n",
    "loss = -probs[torch.arange(5),ys].log().mean()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bckward pass\n",
    "W.grad = None # set grad to zero\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Considering entire dataset\n",
    "xs, ys = [],[]\n",
    "\n",
    "for w in words[:]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn(27,27,generator = g, requires_grad= True)\n",
    "\n",
    "xenc = F.one_hot(xs,num_classes= 27).float()\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "  # Forward Pass\n",
    "  logits = xenc @ W\n",
    "  counts = logits.exp()\n",
    "  probs = counts/counts.sum(1,keepdims= True)\n",
    "\n",
    "  loss = -probs[torch.arange(list(xs.shape)[0]),ys].log().mean() + 0.01*(W**2).mean()\n",
    "  print(f\"Loss on iteration number {i}\",loss.item())\n",
    "  # Backward pass\n",
    "  W.grad = None # set grad to zero\n",
    "  loss.backward()\n",
    "  W.data += -1 * W.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "  # Forward Pass\n",
    "  logits = xenc @ W\n",
    "  counts = logits.exp()\n",
    "  probs = counts/counts.sum(1,keepdims= True)\n",
    "\n",
    "  loss = -probs[torch.arange(list(xs.shape)[0]),ys].log().mean() + 0.1*(W**2).mean()\n",
    "  print(f\"Loss on iteration number {i}\",loss.item())\n",
    "  # Backward pass\n",
    "  W.grad = None # set grad to zero\n",
    "  loss.backward()\n",
    "  W.data += -1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally sampling from neural net model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "  out = []\n",
    "  ix = 0\n",
    "\n",
    "  while True:\n",
    "    xenc = F.one_hot(torch.tensor([ix]),num_classes= 27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    p = counts/counts.sum(1,keepdims= True)\n",
    "    ix = torch.multinomial(p,num_samples=1,replacement=True,generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
